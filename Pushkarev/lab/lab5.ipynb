{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adagrad\n",
    "from keras.losses import MeanSquaredError, MeanAbsoluteError, Huber, LogCosh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv('./data/temp.csv')\n",
    "stock = pd.read_csv('./data/TSLA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = stock.iloc[:1000]\n",
    "stock = stock[['close', 'time']]\n",
    "stock['time'] = pd.to_datetime(stock['time'])\n",
    "stock = stock.set_index('time')\n",
    "\n",
    "temp = temp[['date', 'meantemp']]\n",
    "temp['date'] = pd.to_datetime(temp['date'])\n",
    "temp = temp.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(stock) * 0.8)\n",
    "train, test = stock[0:train_size], stock[train_size:len(stock)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_dim=1))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [MeanSquaredError(), MeanAbsoluteError(), Huber(), LogCosh()]\n",
    "optimizers = [Adam(), SGD(), RMSprop(), Adagrad()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшая модель имеет loss: 0.07295605540275574\n"
     ]
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "best_model = None\n",
    "\n",
    "best_loss_name = None\n",
    "best_optim_name = None\n",
    "\n",
    "\n",
    "for loss in losses:\n",
    "    for optimizer in optimizers:\n",
    "        model.compile(loss=loss, optimizer=optimizer)\n",
    "        model.fit(train, train, epochs=50, verbose=0)\n",
    "        \n",
    "        # Оценка модели\n",
    "        test_loss = model.evaluate(test, test, verbose=0)\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            best_model = model\n",
    "            \n",
    "\n",
    "print(\"Лучшая модель имеет loss:\", best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<keras.src.optimizers.adagrad.Adagrad at 0x1a848e91390>,\n",
       " <keras.src.losses.LogCosh at 0x1a849f6dd80>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.optimizer, best_model.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(temp) * 0.8)\n",
    "train, test = temp[0:train_size], temp[train_size:len(temp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшая модель имеет loss: 5.1132988929748535\n"
     ]
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "best_model = None\n",
    "\n",
    "best_loss_name = None\n",
    "best_optim_name = None\n",
    "\n",
    "\n",
    "for loss in losses:\n",
    "    for optimizer in optimizers:\n",
    "        model.compile(loss=loss, optimizer=optimizer)\n",
    "        model.fit(train, train, epochs=50, verbose=0)\n",
    "        \n",
    "        # Оценка модели\n",
    "        test_loss = model.evaluate(test, test, verbose=0)\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            best_model = model\n",
    "            \n",
    "\n",
    "print(\"Лучшая модель имеет loss:\", best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<keras.src.optimizers.adagrad.Adagrad at 0x1a848e91390>,\n",
       " <keras.src.losses.LogCosh at 0x1a849f6dd80>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.optimizer, best_model.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На обоих наборах данных лучшими покуказали себя Adagrad и LogCosh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Adagrad в целом считается самым гибким и лучшим оптимизатором, который стоит использовать в 99% случаев, поэтому данный результат очевиден\n",
    "* Опыта работы ранее с LogCosh не имел, поэтому выводы делаю исключительно из теоретического материала. Во первых, LogCosh менее чувствительна к выбросам, чем другие метрики, что для данных котировок очень важно. Также имеет особенность к сглаживанию ошибок, а также устойчива численно, то есть не уходит так быстро в бесконечность, как например MSE. Также LogCosh дифференцируема"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
